{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO048cxLqyeynizgvbqGpVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketSanyal/DifferentialPrivacy/blob/main/Full_Fine_Tuning_DP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "IYWb-kh9P4ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers opacus"
      ],
      "metadata": {
        "id": "3iv5divMX8Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"prajjwal1/bert-tiny\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "class Config:\n",
        "  num_train_epochs = 7\n",
        "  learning_rate = 5e-4\n",
        "  n_prompt_tokens = 10\n",
        "  random_range  = 0.5\n",
        "  batch_size = 256\n",
        "  max_grad_norm = 0.1\n",
        "args = Config()\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"sentence\"], max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "ssyS_qH5R-0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "fv8nIvmySIXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n,p in model.named_parameters():\n",
        "  if p.shape == (128,128):\n",
        "    print(n)"
      ],
      "metadata": {
        "id": "pXD-ZbMHb9FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, p in enumerate(model.parameters()):\n",
        "    print(p.shape)"
      ],
      "metadata": {
        "id": "3HMkXBXIbtwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_layers = [5,7,9,11,21,23,25,27,37,39,40]\n",
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i in trainable_layers:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "IEK1k-0_dfL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.named_parameters())"
      ],
      "metadata": {
        "id": "7hgzmhKud3w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n,p in model.named_parameters():\n",
        "  print(n, \"shape :\", p.shape)"
      ],
      "metadata": {
        "id": "PxobZXDScNlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cuda()\n",
        "model.train()"
      ],
      "metadata": {
        "id": "bpHWb42JSSe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "RTbLTLZtVstn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate(batch):\n",
        "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n"
      ],
      "metadata": {
        "id": "9Mxi8szOV00Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DELTA = 1 / len(train_dataloader)\n",
        "\n",
        "LOGGING_INTERVAL = 100"
      ],
      "metadata": {
        "id": "fx3aNzDDYeyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, torch\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "\n",
        "privacy_engine = PrivacyEngine()\n",
        "model.train()\n",
        "model, optimiser, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_dataloader,\n",
        "    target_delta=DELTA,\n",
        "    target_epsilon=8,\n",
        "    epochs=args.num_train_epochs,\n",
        "    max_grad_norm=args.max_grad_norm,\n",
        ")"
      ],
      "metadata": {
        "id": "h10CiZAJSlJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "# define evaluation cycle\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "\n",
        "    loss_arr = []\n",
        "    accuracy_arr = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "        labels =batch['labels'].detach().cpu().numpy()\n",
        "\n",
        "        loss_arr.append(loss.item())\n",
        "        accuracy_arr.append(accuracy(preds, labels))\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(loss_arr), np.mean(accuracy_arr)\n",
        ""
      ],
      "metadata": {
        "id": "Vczzn6mUZCVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")\n",
        "device=\"cuda\""
      ],
      "metadata": {
        "id": "uy6ORcLRZKai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, args.num_train_epochs+1):\n",
        "    losses = []\n",
        "\n",
        "    for  step, batch in enumerate(train_dataloader):\n",
        "            optimiser.zero_grad()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch) # output = loss, logits, hidden_states, attentions\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            optimiser.step()\n",
        "\n",
        "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "                train_loss = np.mean(losses)\n",
        "                eps = privacy_engine.get_epsilon(DELTA)\n",
        "                eval_loss, eval_accuracy = evaluate(model)\n",
        "\n",
        "                print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {step} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                  f\"É›: {eps:.2f}\"\n",
        "                )"
      ],
      "metadata": {
        "id": "SBBrRMK2Y00S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOR QNLI"
      ],
      "metadata": {
        "id": "7S1V3CUre-yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"qnli\")\n",
        "\n",
        "\n",
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(5000)])\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question\"],example[\"sentence\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'sentence', 'question'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "Vhr2775ie_qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "cXEV-p9-fEpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_layers = [5,7,9,11,21,23,25,27,37,39,40]\n",
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i in trainable_layers:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "eiSgqUqofLs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params"
      ],
      "metadata": {
        "id": "gMDB1EHnhVRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "DdWYVfBsfL4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DELTA = 1 / len(train_dataloader)\n",
        "\n",
        "LOGGING_INTERVAL = 100"
      ],
      "metadata": {
        "id": "Ge_DhA96fN2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "privacy_engine = PrivacyEngine()\n",
        "model.train()\n",
        "model, optimiser, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_dataloader,\n",
        "    target_delta=DELTA,\n",
        "    target_epsilon=8,\n",
        "    epochs=args.num_train_epochs,\n",
        "    max_grad_norm=args.max_grad_norm,\n",
        ")"
      ],
      "metadata": {
        "id": "VWU2IvCgfQKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, args.num_train_epochs+1):\n",
        "    losses = []\n",
        "\n",
        "    for  step, batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch) # output = loss, logits, hidden_states, attentions\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "                train_loss = np.mean(losses)\n",
        "                #eps = privacy_engine.get_epsilon(DELTA)\n",
        "                eval_loss, eval_accuracy = evaluate(model)\n",
        "\n",
        "                print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {step} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                  f\"É›: {eps:.2f}\"\n",
        "                )"
      ],
      "metadata": {
        "id": "Qv_uZjsufTO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QQP"
      ],
      "metadata": {
        "id": "iLWPATV1he3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"qqp\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(2000)])\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question1\"],example[\"question2\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'question1', 'question2'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "zZGOP1GBhfcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del optimiser"
      ],
      "metadata": {
        "id": "-8sWhiNdi4gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "-JSn3tUUhqaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_layers = [5,7,9,11,21,23,25,27,37,39,40]\n",
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i in trainable_layers:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "VYXysLOWhqaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "6REV58rlhqaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DELTA = 1 / len(train_dataloader)\n",
        "\n",
        "LOGGING_INTERVAL = 100"
      ],
      "metadata": {
        "id": "MIwQPtgvhqaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")\n",
        "device=\"cuda\""
      ],
      "metadata": {
        "id": "SdFfT4qwhqaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "privacy_engine = PrivacyEngine()\n",
        "model.train()\n",
        "model, optimiser, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_dataloader,\n",
        "    target_delta=DELTA,\n",
        "    target_epsilon=8,\n",
        "    epochs=args.num_train_epochs,\n",
        "    max_grad_norm=args.max_grad_norm,\n",
        ")"
      ],
      "metadata": {
        "id": "EBoVADP9hqaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "for epoch in range(1, args.num_train_epochs+1):\n",
        "    losses = []\n",
        "\n",
        "    with BatchMemoryManager(\n",
        "        data_loader=train_dataloader,\n",
        "        max_physical_batch_size=32,\n",
        "        optimizer=optimiser\n",
        "    ) as memory_safe_data_loader:\n",
        "        for step, batch in enumerate(memory_safe_data_loader):\n",
        "            optimiser.zero_grad()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch) # output = loss, logits, hidden_states, attentions\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            optimiser.step()\n",
        "\n",
        "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "                train_loss = np.mean(losses)\n",
        "                eps = privacy_engine.get_epsilon(DELTA)\n",
        "                eval_loss, eval_accuracy = evaluate(model)\n",
        "\n",
        "                print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {step} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                  f\"É›: {eps:.2f}\"\n",
        "                )"
      ],
      "metadata": {
        "id": "bwqrCd1XhqaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNLI"
      ],
      "metadata": {
        "id": "ZEenOaXAkvWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation_matched'] = raw_datasets['validation_matched'].select([i for i in range(5000)])\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, num_labels =3)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"premise\"],example[\"hypothesis\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'premise', 'hypothesis'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation_matched\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "oXRx0Wt2kwJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
      ],
      "metadata": {
        "id": "xtCfl6XAk9jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_layers = [5,7,9,11,21,23,25,27,37,39,40]\n",
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i in trainable_layers:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "zQSzsto3k9jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "6_Tzesg0k9jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DELTA = 1 / len(train_dataloader)\n",
        "\n",
        "LOGGING_INTERVAL = 100"
      ],
      "metadata": {
        "id": "W5ZlCLTTk9jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")\n",
        "device=\"cuda\""
      ],
      "metadata": {
        "id": "_kttw2Wmk9jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "privacy_engine = PrivacyEngine()\n",
        "model.train()\n",
        "model, optimiser, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_dataloader,\n",
        "    target_delta=DELTA,\n",
        "    target_epsilon=8,\n",
        "    epochs=args.num_train_epochs,\n",
        "    max_grad_norm=args.max_grad_norm,\n",
        ")"
      ],
      "metadata": {
        "id": "WFq3SaEtk9jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "for epoch in range(1, args.num_train_epochs+1):\n",
        "    losses = []\n",
        "\n",
        "    with BatchMemoryManager(\n",
        "        data_loader=train_dataloader,\n",
        "        max_physical_batch_size=32,\n",
        "        optimizer=optimiser\n",
        "    ) as memory_safe_data_loader:\n",
        "        for step, batch in enumerate(memory_safe_data_loader):\n",
        "            optimiser.zero_grad()\n",
        "            #batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch) # output = loss, logits, hidden_states, attentions\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            optimiser.step()\n",
        "\n",
        "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "                train_loss = np.mean(losses)\n",
        "                eps = privacy_engine.get_epsilon(DELTA)\n",
        "                eval_loss, eval_accuracy = evaluate(model)\n",
        "\n",
        "                print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {step} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                  f\"É›: {eps:.2f}\"\n",
        "                )"
      ],
      "metadata": {
        "id": "elMwdiELk9jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WMbZbnDlJDa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}