{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdd4w8gZgy8kKcY2BBh7K1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketSanyal/DifferentialPrivacy/blob/main/Full_Fine_Tuning_Prefix_Tuning_Last_Layer_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus transformers peft datasets"
      ],
      "metadata": {
        "id": "yTX1XX4hUNer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMH8yiluP1Do"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"glue\", \"sst2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaOyoe-_RDsw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"prajjwal1/bert-tiny\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyNzoBUpUYHk"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "  num_train_epochs = 7\n",
        "  learning_rate = 1e-3\n",
        "  n_prompt_tokens = 10\n",
        "  random_range  = 0.5\n",
        "  batch_size = 256\n",
        "  max_grad_norm = 0.1\n",
        "args = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34nBdLVU8hTZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"sentence\"], max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jVLJPQw4Ky1"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate(batch):\n",
        "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-DynuZN4PNL"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77YWgWqGTyfT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC7hDM6aUIEg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_soft_prompt_model(num_labels):\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "VdFA5Wqfdz8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =get_new_soft_prompt_model(2)"
      ],
      "metadata": {
        "id": "zdAWNjIzjDwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "# define evaluation cycle\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "\n",
        "    loss_arr = []\n",
        "    accuracy_arr = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "        labels =batch['labels'].detach().cpu().numpy()\n",
        "\n",
        "        loss_arr.append(loss.item())\n",
        "        accuracy_arr.append(accuracy(preds, labels))\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(loss_arr), np.mean(accuracy_arr)"
      ],
      "metadata": {
        "id": "XXaXGbzhjPw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y7oTb35UbJg"
      },
      "outputs": [],
      "source": [
        "total_params = 0\n",
        "for p in model.parameters():\n",
        "    total_params += p.numel()\n",
        "print(total_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "8xsbLl_ajHRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOGGING_INTERVAL = 100"
      ],
      "metadata": {
        "id": "tzUiwJZFju6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device= \"cuda\""
      ],
      "metadata": {
        "id": "F9qIAr9bWSns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_full_finetune(model,optimiser, train_dataloader):\n",
        "  for epoch in range(1, args.num_train_epochs+1):\n",
        "    losses = []\n",
        "\n",
        "    for  step, batch in enumerate(train_dataloader):\n",
        "            optimiser.zero_grad()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch) # output = loss, logits, hidden_states, attentions\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            optimiser.step()\n",
        "\n",
        "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "                train_loss = np.mean(losses)\n",
        "\n",
        "                eval_loss, eval_accuracy = evaluate(model)\n",
        "\n",
        "                print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {step} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                )"
      ],
      "metadata": {
        "id": "gaITSoP-jg9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model, optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "dCmn5fWLj_oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOR QNLI"
      ],
      "metadata": {
        "id": "c36fY-RI-8RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"qnli\")"
      ],
      "metadata": {
        "id": "sOlzO_9s-9Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(5000)])"
      ],
      "metadata": {
        "id": "n08yRDJN-__x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question\"],example[\"sentence\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'sentence', 'question'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "92gRrd8k_EOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "MEIf8WLL_GeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_new_soft_prompt_model(2)\n",
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "e27nLGtt_I0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model, optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "AKhRbkAu_ff7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QQP"
      ],
      "metadata": {
        "id": "jygrwm6oFb6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"qqp\")"
      ],
      "metadata": {
        "id": "bPeKTqeoFd3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(5000)])"
      ],
      "metadata": {
        "id": "SM_lNi92FfzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][:5]"
      ],
      "metadata": {
        "id": "sHxTtOC6F5WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question1\"],example[\"question2\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'question1', 'question2'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "MnTQeideFjo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "9cXowf-gGFWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_new_soft_prompt_model(2)\n",
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "N6OjaqUMF3zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model, optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "TIcFxKwzGI3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbIWLQ77GNT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For MNLI"
      ],
      "metadata": {
        "id": "YbETvcbAGYF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"mnli\")"
      ],
      "metadata": {
        "id": "rILdToDbGZcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation_matched'] = raw_datasets['validation_matched'].select([i for i in range(5000)])\n"
      ],
      "metadata": {
        "id": "91G4FmVhGcf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, num_labels =3)"
      ],
      "metadata": {
        "id": "QYvJ3VpeGzQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"premise\"],example[\"hypothesis\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'premise', 'hypothesis'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "tbQCTe-yGfoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation_matched\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "ehRpOFLEGsx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_new_soft_prompt_model(3)\n",
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "Ea5uZt5CG7V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "50emVeiyWL2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model, optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "gr7ZImBfHEOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last Layer Fine Tuning"
      ],
      "metadata": {
        "id": "MeBc5lBwU0YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"sst2\")"
      ],
      "metadata": {
        "id": "NNEfReiCU10M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n"
      ],
      "metadata": {
        "id": "gVF-4iBJXIsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =get_new_soft_prompt_model(2)"
      ],
      "metadata": {
        "id": "xtQb-H1AXM9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"sentence\"], max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "KXuGUkELX0vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "-44lD9-QXkSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i==40  or i==39:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "fMhW_TWdX_xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params"
      ],
      "metadata": {
        "id": "jEpucS4SZtKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters())"
      ],
      "metadata": {
        "id": "WPelG-UpYhmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "VAFxg5Z_Ywzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "6Prh4o6iZGTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "Nvp4_vfIY_N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "qC4NpgSWZDKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QQP"
      ],
      "metadata": {
        "id": "FtUe2lKocJbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"qqp\")"
      ],
      "metadata": {
        "id": "TRsIOn9gaMxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(5000)])"
      ],
      "metadata": {
        "id": "ogDpbdNTaMxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question1\"],example[\"question2\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'question1', 'question2'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "rkKTNMV_aMxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "asgZIDlEaMxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =get_new_soft_prompt_model(2)\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "SQK7DrYCaeqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i==40  or i==39:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "j24wPedEaO17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "LLIiN7xvak3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "sTq7rMKpaw3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QNLI"
      ],
      "metadata": {
        "id": "2u7R91MrcMGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"qnli\")"
      ],
      "metadata": {
        "id": "EwbJzvWebN7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(5000)])"
      ],
      "metadata": {
        "id": "MSIvzwGWbN7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question\"],example[\"sentence\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'sentence', 'question'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "jyFEZUoMbN7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "iRvfKw8MbN7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =get_new_soft_prompt_model(2)\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "tmo3v1TabSht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i==40  or i==39:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "EOKCxQPsbsrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "bk0cadKzbwmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "aBKLnppNbz2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNLI"
      ],
      "metadata": {
        "id": "fFObIKAXcN5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"mnli\")"
      ],
      "metadata": {
        "id": "_XN2Ap6tcoNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation_matched'] = raw_datasets['validation_matched'].select([i for i in range(5000)])\n"
      ],
      "metadata": {
        "id": "s1nuDd-mcoNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, num_labels =3)"
      ],
      "metadata": {
        "id": "4KRD7WZ1coN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"premise\"],example[\"hypothesis\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'premise', 'hypothesis'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "zZdTsvd9coN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation_matched\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "hvGOWfZgcoN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_new_soft_prompt_model(3)\n",
        "trainable_params = 0\n",
        "for i, p in enumerate(model.parameters()):\n",
        "  if i==40  or i==39:\n",
        "    trainable_params += p.numel()\n",
        "    p.requires_grad =True\n",
        "  else:\n",
        "    p.requires_grad = False\n",
        "model.to(\"cuda\")\n",
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "gOPkcbwPcoN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "D8n80NxQcvd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 4):\n",
        "    losses = []\n",
        "\n",
        "    for  step, batch in enumerate(train_dataloader):\n",
        "            optimiser.zero_grad()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch) # output = loss, logits, hidden_states, attentions\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            optimiser.step()\n",
        "\n",
        "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "                train_loss = np.mean(losses)\n",
        "\n",
        "                eval_loss, eval_accuracy = evaluate(model)\n",
        "\n",
        "                print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {step} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                )"
      ],
      "metadata": {
        "id": "ppMpE7B-cyzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuyaKkVjdYJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREFIX TUNING"
      ],
      "metadata": {
        "id": "zOs1seKddZSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType"
      ],
      "metadata": {
        "id": "Lox_pqmKdadI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FApQbD-fd_w5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"glue\", \"sst2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJp_7uMTd_xA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"prajjwal1/bert-tiny\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh9ao33Qd_xA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"sentence\"], max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLj2cDRjd_xA"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del optimiser"
      ],
      "metadata": {
        "id": "adt_EYrDeXEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  AutoModelForSeq2SeqLM\n",
        "\n",
        "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, num_virtual_tokens=10)\n",
        "model = get_new_soft_prompt_model(2)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "tc3qFjneeBHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)"
      ],
      "metadata": {
        "id": "2rP2XZ-flmB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "Nh72-N5JnZrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "oXg1CKfTeLvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For QNLI"
      ],
      "metadata": {
        "id": "kK5Emcq3n1uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"qnli\")"
      ],
      "metadata": {
        "id": "sWXmyvb0n_t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(5000)])"
      ],
      "metadata": {
        "id": "VajcT061n_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question\"],example[\"sentence\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'sentence', 'question'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "fm_n79BNn_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "tHPaGommn_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, num_virtual_tokens=10)\n",
        "model = get_new_soft_prompt_model(2)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "_mnAlFaVoBVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "x-nn1lZnohJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QQP"
      ],
      "metadata": {
        "id": "FHD-du9qpONp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"qqp\")"
      ],
      "metadata": {
        "id": "pzGURo_CpXhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation'] = raw_datasets['validation'].select([i for i in range(5000)])\n",
        "\n",
        "raw_datasets['test'] = raw_datasets['test'].select([i for i in range(5000)])"
      ],
      "metadata": {
        "id": "E8-tSPuZpXhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"question1\"],example[\"question2\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'question1', 'question2'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "1RCfjC5FpXhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "ZszGDV50pXhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, num_virtual_tokens=10)\n",
        "model = get_new_soft_prompt_model(2)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "optimiser = AdamW(model.parameters(), lr=args.learning_rate)\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "xN8YW8cUpYzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "Kf4WDuTkpcAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNLI"
      ],
      "metadata": {
        "id": "XNiIxtNHpudK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"mnli\")"
      ],
      "metadata": {
        "id": "UB8sv7Mhp6Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'] = raw_datasets['train'].select([i for i in range(50000)])\n",
        "\n",
        "raw_datasets['validation_matched'] = raw_datasets['validation_matched'].select([i for i in range(5000)])\n"
      ],
      "metadata": {
        "id": "to5WwOZnp6Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, num_labels =3)"
      ],
      "metadata": {
        "id": "WkBbxQz5p6Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "tokenized_dataset = raw_datasets.map(\n",
        "    lambda example: tokenizer(example[\"premise\"],example[\"hypothesis\"] ,max_length=64, padding='max_length', truncation=True),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['idx', 'premise', 'hypothesis'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "hiZqtHIfp6Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"validation_matched\"], shuffle=False, batch_size=args.batch_size, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "iBz7lQrHp6Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, num_virtual_tokens=10)\n",
        "model = get_new_soft_prompt_model(3)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "mnuAE_mtp8qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_finetune(model,optimiser, train_dataloader)"
      ],
      "metadata": {
        "id": "9Ab5I4cjqA1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CHieycD7qERY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}